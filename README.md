# Automated Motion Canvas Video Pipeline

This project converts a topic into a Motion Canvas scene file using a structured pipeline:

`Idea -> LLM -> VideoSpec JSON -> Director refinement -> Motion Canvas scene code`

The single source of truth is the validated `VideoSpec` object.

## Current Process (Exact)

1. Run generation CLI.
2. `scriptAgent` asks the model for strict JSON (`json_schema` response format).
3. Raw LLM `VideoSpec` is saved to `output/videospec.llm.json`.
4. `structuredOutput` parses + validates with Zod (`videoSpecSchema`).
5. `directorAgent` enforces deterministic pacing and timing rules.
6. `motionCanvasAgent` converts to `renderSpec`.
7. `renderer` writes `src/scenes/generatedPipelineScene.tsx`.
8. Motion Canvas project (`src/project.ts`) imports and uses `generatedPipelineScene`.

## Important Clarification

You asked whether code is needed after `renderSpec`.

No manual code is needed.

`src/scenes/generatedPipelineScene.tsx` already contains:
- `const renderSpec = {...}` data
- executable scene generator code below it (`makeScene2D(...)`) that loops scenes/elements and applies animations

That runtime code is auto-generated by `src/motion/renderer.ts`.

## Setup

1. Install dependencies:
```bash
npm install
```

2. Configure environment:
```bash
cp .env.example .env
```
Set:
- `OPENAI_API_KEY=...`
- `OPENAI_MODEL=gpt-5.2` (or omit to use default)

## Generate Scene Code

Default example topic:
```bash
npm run generate
```

Custom topic/audience/duration:
```bash
npm run generate -- \
  --topic "How systems scale to 1 million users" \
  --audience "beginner" \
  --duration 60 \
  --model gpt-5.2 \
  --output src/scenes/generatedPipelineScene.tsx \
  --llm-spec-output output/videospec.llm.json
```

Output:
- Generated scene file path printed in terminal.
- Usually `src/scenes/generatedPipelineScene.tsx`.
- Raw LLM output spec is saved to `output/videospec.llm.json` by default.

## V2 Pipeline Usage

The v2 pipeline supports:
- StoryIntent agent -> Topology agent -> composition/layout/compile -> generated scene
- Optional manual topology input for fast local iteration (no LLM calls)

### V2 without params (defaults)

```bash
npm run generate:v2
```

Defaults used:
- `topic`: `How systems scale to 1 million users`
- `audience`: `beginner`
- `duration`: `60`
- `pipeline`: `v2`
- `stableLayout`: `true` (v2 default)
- `animate`:
  - `false` in LLM mode (unless `--animate` is passed)
  - `true` in manual topology mode (`--topology-input ...`)

### V2 with params (LLM mode)

```bash
npm run generate:v2 -- \
  --topic "How to scale an app to 1M users" \
  --audience "beginner" \
  --duration 60 \
  --model gpt-5.2 \
  --animate \
  --stable-layout \
  --personality ENERGETIC \
  --output src/scenes/generatedPipelineScene.tsx \
  --story-intent-output output/v2/story-intent.llm.json \
  --topology-output output/v2/topology.llm.json
```

### V2 manual topology mode (no LLM)

Use your own topology JSON for fast iteration:

```bash
npm run generate:v2 -- \
  --topology-input output/v2/topology.manual.json \
  --animate \
  --stable-layout
```

Optional: provide a matching story intent file (used for tone/personality mapping and artifact output):

```bash
npm run generate:v2 -- \
  --topology-input output/v2/topology.manual.json \
  --story-intent-input output/v2/story-intent.llm.json \
  --animate \
  --stable-layout
```

There is also a convenience script:

```bash
npm run generate:v2:local
```

### V2 CLI flags

All flags accepted by `src/cli/generate.ts` for v2:

- `--topic <string>`
- `--audience <string>`
- `--duration <number>`
- `--model <string>`
- `--output <path>`
- `--story-intent-output <path>`
- `--topology-output <path>`
- `--topology-input <path>`
- `--story-intent-input <path>`
- `--personality <CALM|ENERGETIC|PREMIUM>`
- `--animate` (flag)
- `--stable-layout` (flag)

### V2 outputs and artifacts

Standard outputs:
- `src/scenes/generatedPipelineScene.tsx`
- `output/v2/story-intent.llm.json` (LLM mode or provided path)
- `output/v2/topology.llm.json`
- `output/v2/composition.debug.json`
- `output/v2/layout.debug.json`
- `output/v2/moments.designed.json`
- `output/v2/renderspec.debug.json`

Run history snapshots (per run):
- `output/history/v2/<run_id>/...`

## Create Video (Render)

1. Start Motion Canvas studio:
```bash
npm run studio
```

2. Open the project in browser.
3. Ensure `src/project.ts` includes `generatedPipelineScene` (already wired).
4. Use the Motion Canvas UI render/export action to render MP4/GIF.

## Key Files

- Pipeline orchestrator: `src/pipeline/generateVideo.ts`
- Prompt: `src/llm/prompts.ts`
- Structured output + retries: `src/llm/structuredOutput.ts`
- OpenAI client: `src/llm/openaiClient.ts`
- VideoSpec schema + strict response format: `src/schema/videoSpec.schema.ts`
- Scene template writer: `src/motion/renderer.ts`
- Generated scene: `src/scenes/generatedPipelineScene.tsx`
- Motion Canvas project entry: `src/project.ts`

## Troubleshooting

- `Invalid schema for response_format`: check `videoSpecResponseFormat` in `src/schema/videoSpec.schema.ts`.
- Parse/validation failures: inspect logs from `structuredOutput` for issue paths.
- Network errors (`EAI_AGAIN`): DNS/network issue while calling OpenAI API.
